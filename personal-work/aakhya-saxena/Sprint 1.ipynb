{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064e9592-ccc0-4ebb-bbaf-9726e41d0b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scraping comments from multiple EV subreddits...\n",
      " Scraped 5000 comments from multiple subreddits and saved to ev_comments.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Reddit connection\n",
    "reddit = praw.Reddit(\n",
    "    client_id='ZvOmvXHhCkKENkIqvRmkgg',\n",
    "    client_secret='IZbNrLtd5ZjkgbsyhlNEjx6G_-RUnw',\n",
    "    user_agent='EVSentimentAnalysisBot v1.0'\n",
    ")\n",
    "\n",
    "# Combine subreddits\n",
    "subreddits = reddit.subreddit(\"electricvehicles+TeslaMotors+electriccars+EVs+ElectricVehiclesEU\")\n",
    "\n",
    "# Config\n",
    "target_comment_count = 5000\n",
    "all_comments = []\n",
    "post_limit = 2000  # Total posts to scan across all subreddits\n",
    "\n",
    "print(\" Scraping comments from multiple EV subreddits...\")\n",
    "\n",
    "for post in subreddits.top(limit=post_limit):\n",
    "    post.comments.replace_more(limit=0)\n",
    "    for comment in post.comments:\n",
    "        if len(all_comments) >= target_comment_count:\n",
    "            break\n",
    "        body = comment.body.replace('\\n', ' ').strip()\n",
    "        if len(body) > 0:\n",
    "            all_comments.append([\n",
    "                post.subreddit.display_name,\n",
    "                post.id,\n",
    "                post.title,\n",
    "                comment.id,\n",
    "                body\n",
    "            ])\n",
    "\n",
    "    if len(all_comments) >= target_comment_count:\n",
    "        break\n",
    "\n",
    "    time.sleep(1)  # Pause between posts to avoid being blocked\n",
    "\n",
    "# Save to CSV\n",
    "with open('ev_comments.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['subreddit', 'post_id', 'post_title', 'comment_id', 'comment_body'])\n",
    "    writer.writerows(all_comments)\n",
    "\n",
    "print(f\" Scraped {len(all_comments)} comments from multiple subreddits and saved to ev_comments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aedfb54f-d6fa-464a-8a4b-b4b209baa2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned and saved 3903 comments to ev_comments_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('ev_comments.csv')\n",
    "\n",
    "# Cleaning function\n",
    "def clean_comment(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-letters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['clean_comment'] = df['comment_body'].astype(str).apply(clean_comment)\n",
    "\n",
    "# Remove comments that are too short (e.g. < 5 words)\n",
    "df = df[df['clean_comment'].str.split().str.len() >= 5]\n",
    "\n",
    "# Save cleaned version\n",
    "df.to_csv('ev_comments_cleaned.csv', index=False)\n",
    "\n",
    "print(f\" Cleaned and saved {len(df)} comments to ev_comments_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03720cb5-112e-408a-ab82-574eae7e27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(\"ev_comments_cleaned.csv\")\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"clean_comment\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a34c90-0530-41ef-9e34-6caa34c562cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3903 entries, 0 to 3902\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   subreddit      3903 non-null   object\n",
      " 1   post_id        3903 non-null   object\n",
      " 2   post_title     3903 non-null   object\n",
      " 3   comment_id     3903 non-null   object\n",
      " 4   comment_body   3903 non-null   object\n",
      " 5   clean_comment  3903 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 183.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
